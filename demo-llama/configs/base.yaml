model:
  d_model: 4096
  n_heads: 32
  n_layers: 32
  preset: llama
  vocab_size: 50000
training:
  batch_size: 512
  lr: 0.0003
  max_steps: 100000
  warmup_steps: 2000
